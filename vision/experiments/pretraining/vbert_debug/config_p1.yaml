data_param:
    ocr:
        dataset_name: OCR
        training_datasets_paths: paultltc/smolarxiv-ocr
        add_begin_of_doc_token: True
        add_end_of_doc_token: True
        map_batch_size: 64
        max_num_images: 3
        max_seq_len: 8192
        pad_dataset: True
        # shuffle_initial_urls_list: True
        # shuffle_before_split_by_node_buffer_size: 1500
        # shuffle_before_split_by_worker_buffer_size: 1500
        # shuffle_after_tarfile_to_samples_buffer_size: 2000
        shuffle_after_packing: True
        max_image_size: 2048
        scale_up_max: 2.0
        scale_up_frequency: 0.0
        pre_split_scale_up_max: 2.0
        pre_split_scale_up_frequency: 1.0
        image_column: image
        text_column: text
    num_workers: 1
    realtime_processing: True
    persistent_workers: True
    pin_memory: True
    proba_interleaving_dataset: [1.0]
    use_webdataset: False
    mask_type: MLM
hparams:
    tokenizer_name: EuroBERT/EuroBERT-210m
    tokenizer_params: '{"use_fast": True}'
    tokenizer_add_tokens: '[AddedToken("<global-img>", rstrip=False, lstrip=False, normalized=False), 
        AddedToken("<row_1_col_1>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_1_col_2>", rstrip=False, lstrip=False, normalized=False), 
        AddedToken("<row_1_col_3>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_1_col_4>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_1_col_5>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_1_col_6>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_2_col_1>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_2_col_2>", rstrip=False, lstrip=False, normalized=False),   
        AddedToken("<row_2_col_3>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_2_col_4>", rstrip=False, lstrip=False, normalized=False), 
        AddedToken("<row_2_col_5>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_2_col_6>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_3_col_1>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_3_col_2>", rstrip=False, lstrip=False, normalized=False), 
        AddedToken("<row_3_col_3>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_3_col_4>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_3_col_5>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_3_col_6>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_4_col_1>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_4_col_2>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_4_col_3>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_4_col_4>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_4_col_5>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_4_col_6>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_5_col_1>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_5_col_2>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_5_col_3>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_5_col_4>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_5_col_5>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_5_col_6>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_6_col_1>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_6_col_2>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_6_col_3>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_6_col_4>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<row_6_col_5>", rstrip=False, lstrip=False, normalized=False), AddedToken("<row_6_col_6>", rstrip=False, lstrip=False, normalized=False),
        AddedToken("<fake_token_around_image>", rstrip=False, lstrip=False, normalized=False), AddedToken("<image>", rstrip=False, lstrip=False, normalized=False),
    ]'
    tokenizer_add_special_tokens: '{}'
    model_name: EuroBERT/EuroBERT-210m
    model_config:
        vision_config:
            vision_model_name: google/siglip-base-patch16-512  # /fsx/hugo/siglip-so400m-14-364-flash-attn2  # HuggingFaceM4/siglip-so400m-14-364-flash-attn2 # google/siglip-so400m-patch14-384
        text_config:
            text_model_name: EuroBERT/EuroBERT-210m
        freeze_config:
            freeze_vision_layers: True
            freeze_text_layers: True
            freeze_lm_head: True
        _attn_implementation: "flash_attention_2"
        tie_word_embeddings: False
        qk_layer_norms: False
        use_cache: True
        neftune_noise_alpha: 0.0
        image_token_id: 128294
    lora_config:
        lora_alpha: 16
        lora_dropout: 0.1
        r: 64
        bias: "none"
        init_lora_weights: "gaussian"
        use_dora: True
    use_lora: True
    patterns_to_loraify: [
        ["vision", "encoder", "q_proj"],
        ["vision", "encoder", "k_proj"],
        ["vision", "encoder", "v_proj"],
        ["vision", "encoder", "out_proj"],
        ["vision", "encoder", "mlp"],
        ["model.layers", "proj"],
        ["lm_head"],
    ]
    patterns_to_unfreeze: [["modality"], ["vision", "embed"], ["norm"], ["model", "embed_tokens", "additional_embedding"], ["additional_fc"]]
    global_batch_size: 1
    batch_size_per_gpu: 1
    gradient_checkpointing: True
    grad_clip: 1.0
    max_num_opt_steps: 1000
    seed: 42
    # train_logging_activations:
    # - jsonl
    train_logging_activations_opt_steps: 250
    # train_logging_grad_param_deepspeed:
    # - jsonl
    train_logging_grad_param_deepspeed_opt_steps: 250
    train_logging_opt_steps: 1
    train_saving_opt_steps: 250
    val_logging_opt_steps: 250
    do_validation: False
    # kill_switch_path: /fsx/m4/experiments/kill-switch-tr_320.txt
    save_dir: /home/paulteiletche/smolvencoder/vision/experiments/pretraining/vbert_debug/checkpoints/vbert_p1
    wandb_enable: true
    wandb_entity: smolvencoder
    wandb_log_freq: 0.1
    wandb_project: debug
    upload_to_s3: False
    timing_break_down: True
optim_param:
    vl_optim: AdamW
    vl_optim_params:
        betas: [0.9, 0.999]
        lr: 0.0001
        weight_decay: 0.1
        no_decay: ["bias", "alpha", "layernorm", "ln", "perceiver_resampler", "layer_norm"]
    vl_lr_scheduler: get_linear_schedule_with_warmup
    vl_lr_scheduler_params:
        last_epoch: -1
        num_warmup_steps: 100
        num_training_steps: 1000
    z_loss: 0